# [Transformer model for end-to-end Machine Translation](https://github.com/amousavi9/Machine-Translation-using-Transformer)
The Transformer model architecture was introduced by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin in their paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762).    
The Transformer model extracts the features for each word using a self-attention mechanism to know the importance of each word in the sentence. No other recurrent units are used to extract this feature, they are just activations and weighted sums, so they can be very efficient and parallelizable.   
In this study, I implement machine translation in Persian language with the help of transformer network by anki dataset.

## Basic Architecture

## Evaluation
